# Count-data GLMMs

# Goals

- Learn to fit poisson and negative binomial GLMMs with various packages in R
- Learn how to criticize GLMM fit
- Learn how to check for overdispersion in GLMMs
- Learn how to interpret and plot output from count GLMMs

We are going to work with one of the data sets from:
Artelle, K. A., S. C. Anderson, J. D. Reynolds, A. B. Cooper, P. C. Paquet, and C. T. Darimont. 2016. Ecology of conflict: marine food supply affects human-wildlife interactions on land. Scientific Reports 6:25936.

Let's read in the data and look at it:

```{r}
library(tidyverse)
d <- readRDS("data/raw/conflict-salmon-for-modelling.rds") %>%
  select(late_kills,
    gbpu,
    salmon_biomass_geo_mean, 
    prev_3yr_conflict_scaled,
    mean_annual_temp_scaled,
    mean_temp_scaled,
    total_log_precip_scaled,
    year_centered,
    log_humanpop_scaled,
    log_grizzly_pop_est_scaled,
    gbpu_usable_area) %>%
  mutate(
    year_scaled = year_centered / (2 * sd(year_centered)),
    gbpu_usable_area = gbpu_usable_area / 1e9) %>%
  rename(salmon_scaled = salmon_biomass_geo_mean)
saveRDS(d, file = "data/generated/artelle-et-all-conflict.rds")
head(d)
```

We are going to use the below formula a number of times. So let's store it in a variable we will call `f`.

```{r}
f <- late_kills ~ 
  salmon_scaled + 
  prev_3yr_conflict_scaled +
  mean_annual_temp_scaled +
  mean_temp_scaled +
  total_log_precip_scaled +
  year_scaled +
  log_humanpop_scaled +
  log_grizzly_pop_est_scaled +
  offset(log(gbpu_usable_area))
```

Let's look at the data in a few ways. 

```{r}
ggplot(d, aes(year_centered, late_kills, color = salmon_scaled)) +
  geom_point() +
  facet_wrap(~gbpu)
```

```{r}
ggplot(d, aes(salmon_scaled, log(late_kills+0.5), color = gbpu)) +
  geom_point(position = position_jitter(height = 0.4, width = 0))
```

```{r}
ggplot(d, aes(salmon_scaled, log((late_kills+0.5)/gbpu_usable_area), 
  color = gbpu)) +
  geom_point(position = position_jitter(height = 0.4, width = 0)) +
  geom_smooth(method = "lm", se = FALSE)
```

```{r}
ggplot(d, aes(salmon_scaled, log((late_kills+0.5)), color = gbpu)) +
  geom_point(position = position_jitter(height = 0.4, width = 0), alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE, colour = "black")
```

```{r}
ggplot(d, aes(salmon_scaled, log((late_kills+0.5)), 
  color = gbpu, group = gbpu)) +
  geom_point(position = position_jitter(height = 0.4, width = 0), 
    alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE, alpha = 0.7, colour = "black")
```

# Modeling 

Let's start by using the lme4 package as we have before. Instead of using the `lmer` function we will use the `glmer` function. (Although `lmer` will work too.)

We have count data. So one obvious place to start is with the Poisson distribution and a log link. 

```{r}
library(lme4)
f_lme4 <- paste(c(f, "+ (1 | gbpu)"), collapse = " ")
f_lme4
m_glmer <- glmer(f_lme4, data = d, family = poisson(link = "log"))
summary(m_glmer)
```

We can look at these default plots, but the default residual plot is not that helpful here. 

```{r}
plot(m_glmer)
lattice::dotplot(ranef(m_glmer, condVar = TRUE))
```

We can also extract fixed effects, random effects, and their standard errors with:

```{r}
fixef(m_glmer)
arm::se.fixef(m_glmer)

ranef(m_glmer)
arm::se.ranef(m_glmer)
```

There are 3 kinds of confidence intervals we can get out of lme4, but only one that will run quickly right now. 

```{r}
confint(m_glmer, method = "Wald")
# confint(m_glmer, method = "profile")
# confint(m_glmer, method = "boot")
```

One useful summary function is 

```{r}
coef(summary(m_glmer))
```

Now we should check for overdispersion.

We can do that by comparing the sum of the person residuals squared with the residual degrees of freedom. 

We can see the degrees of freedom from the output of `summary()`.

And we can calculate the sum of the pearson residuals squared:

```{r}
sum(residuals(m_glmer, "pearson")^2)
aods3::gof(m_glmer)
```

So it looks like we have some overdispersion here.

We can try dealing with this by adding an observation-level random intercept or by switching to a negative binomial distribution. 

Theoretically, we could fit a negative binomial GLMM with the function `lme4::glmer.nb`, but the following fails to converge for me.

```{r}
# m_glmer_nb <- glmer.nb(f_lme4, data = d)
# summary(m_glmer_nb)
```

We could try removing the offset term. 

```{r}
f2 <- late_kills ~ 
  salmon_scaled + 
  prev_3yr_conflict_scaled +
  mean_annual_temp_scaled +
  mean_temp_scaled +
  total_log_precip_scaled +
  year_scaled +
  log_humanpop_scaled +
  log_grizzly_pop_est_scaled

f_lme4_no_offset <- paste(c(f2, "+ (1 | gbpu)"), collapse = " ")
# m_glmer_nb <- glmer.nb(f_lme4_no_offset, data = d)
```

But that doesn't converge for me either.

So instead, let's start by adding an observation-level random intercept. This simply requires making a dummy variable with a level for each row of data:

```{r}
d$obs <- seq_len(nrow(d))
f_lme4_obs <- paste(c(f2, "+ (1 | gbpu) + (1 | obs)"), collapse = " ")
m_lme4_obs <- glmer(f_lme4_obs, data = d, family = poisson(link = "log"))
summary(m_lme4_obs)
aods3::gof(m_lme4_obs)
```

That, too, has poor convergence.

Before we move on to other packages, let's quickly look at some plots we can get out of the sjPlot package. Remember, these confidence intervals shouldn't be trusted because we haven't dealt with overdispersion yet.

```{r}
sjPlot::sjp.glmer(m_glmer)
sjPlot::sjp.glmer(m_glmer, type = "fe")
sjPlot::sjp.glmer(m_glmer, type = "re.qq")
# sjPlot::sjp.glmer(m, type = "ma") # many
p <- sjPlot::sjp.glmer(m_glmer, type = "eff", show.ci = T)
head(p$data)
```

We can try the function `glmmPQL` in the package MASS, which lets us fit the model with something fancy called penalized quasi-likelihood. "Penalized quasi-likelihood" doubles as a great conservation starter at cocktail parties.

```{r}
m_pql <- MASS::glmmPQL(f, random = ~ 1 | gbpu/obs, data = d, 
  family = poisson(link = "log"))
summary(m_pql)
```

This one converges. In the background the function is using nlme. Therefore, we have to work with the object as if it was from nlme. E.g.

```{r}
nlme::intervals(m_pql)
```

What distribution might work best?

```{r}
dg <- group_by(d, gbpu) %>%
  summarise(m = mean(late_kills), v = var(late_kills))
ggplot(dg, aes(m, v)) +
  geom_smooth(method = "lm", formula = y ~ x - 1, se = F) +
  geom_smooth(colour = "red", se = F) +
  geom_smooth(method = "lm", 
    formula = y ~ I(x^2) + offset(x) - 1, colour = "purple", se = F) +
  geom_abline(intercept = 0, slope = 1, lty = 2) +
  geom_point()
```

<!-- b <- broom::augment(m2) -->
<!-- plot(b$.fitted, sqrt(abs(b$.resid))) -->
<!-- plot(m2, gbpu ~ resid(.,type="pearson")) -->

It's always a good idea to fit GLMMs with multiple methods if possible to ensure the conclusions are not sensitive to the optimization method or that your model has not converged.

One of the greatest things since sliced bread is the new TMB package for R. TMB normally requires you to write your own model template, but the package glmmTMB comes with a number of pre-written and tested GLMMs.

TMB, and therefore glmmTMB, is *FAST* and very robust. <https://github.com/glmmTMB>

```{r}
library(glmmTMB)

m_tmb <- glmmTMB(f_lme4, data = d, family = poisson(link = "log"))
summary(m_tmb)

m_tmb_obs <- glmmTMB(f_lme4_obs, data = d, family = poisson(link = "log"))
summary(m_tmb_obs)

m_tmb_nb <- glmmTMB(f_lme4, data = d, family = nbinom2(link = "log"))
summary(m_tmb_nb)

m_tmb_nb1 <- glmmTMB(f_lme4, data = d, family = nbinom1(link = "log"))
summary(m_tmb_nb1)

bbmle::AICtab(m_tmb, m_tmb_nb, m_tmb_nb1)
```

```{r}
plot(log(fitted(m_tmb)), residuals(m_tmb, type = "pearson"))
plot(log(fitted(m_tmb_obs)), residuals(m_tmb_obs, type = "pearson"))
plot(log(fitted(m_tmb_nb)), residuals(m_tmb_nb, type = "pearson"))
plot(log(fitted(m_tmb_nb1)), residuals(m_tmb_nb1, type = "pearson"))
```

The residuals from the negative binomial model look the best to me.

```{r}
plot(log(fitted(m_tmb)), sqrt(abs(residuals(m_tmb, type = "pearson"))))
plot(log(fitted(m_tmb_nb)), sqrt(abs(residuals(m_tmb_nb, type = "pearson"))))
```

Let's plot the residuals by group:

```{r}
d$res_nb <- residuals(m_tmb_nb, type = "pearson")
ggplot(d, aes(gbpu, res_nb)) + geom_boxplot() + coord_flip()
```

Quick Bayesian version:

```{r}
library(rstanarm)
library(rstan)
options(mc.cores = parallel::detectCores())

mstan <- rstanarm::stan_glmer(f_lme4, data = d, 
  family = poisson(link = "log"), chains = 4, iter = 300)
summary(mstan)

mstan_nb <- rstanarm::stan_glmer.nb(f_lme4, data = d, chains = 4,
  iter = 300)

summary(mstan_nb)
stan_plot(mstan_nb)
stan_trace(mstan_nb)
stan_rhat(mstan_nb)
stan_hist(mstan_nb)
stan_est <- broom::tidyMCMC(mstan_nb, conf.int = TRUE, 
  estimate.method = "median")

stan_est %>% filter(grepl("gbpu", term)) %>%
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high)) +
  geom_pointrange() +
  coord_flip()

stan_re_int <- stan_est %>% filter(grepl("gbpu", term))
qqnorm(stan_re_int$estimate)
qqline(stan_re_int$estimate)
```

Let's compare confidence intervals:

```{r}
get_tmb_cis <- function(x, name) {
  ci <- list()
  ci <- data.frame(confint(x), fixef(x)[[1]])
  names(ci) <- c("conf.low", "conf.high", "estimate")
  ci$model <- name
  ci$term <- names(fixef(x)$cond)
  ci
}
cis <- list()
cis[[1]] <- get_tmb_cis(m_tmb, "Poisson")
cis[[2]] <- get_tmb_cis(m_tmb_obs, "Poisson w obs RE")
cis[[3]] <- get_tmb_cis(m_tmb_nb, "Negative binomial")
cis[[4]] <- get_tmb_cis(m_tmb_nb1, "Negative binomial 1")
cis[[5]] <- broom::tidy(m_glmer, conf.int = TRUE) %>%
  dplyr::select(conf.low, conf.high, estimate, term) %>%
  mutate(model = "Poisson (glmer)") %>%
  filter(term != "sd_(Intercept).gbpu")
cis[[6]] <- stan_est %>%
  dplyr::select(conf.low, conf.high, estimate, term) %>%
  filter(!grepl("Intercept", term), term != "overdispersion") %>%
  mutate(model = "Negative binomial (Stan)")

cis_df <- bind_rows(cis) %>% filter(term != "(Intercept)")
```

```{r}
ggplot(cis_df, aes(term, estimate, 
  ymin = conf.low, ymax = conf.high, colour = model)) +
  geom_pointrange(position = position_dodge(width = 0.5)) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = 2)

filter(cis_df, term == "salmon_scaled") %>%
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high, 
    colour = model)) +
  geom_pointrange(position = position_dodge(width = 0.5)) +
  coord_flip() +
  geom_hline(yintercept = 0, linetype = 2)
```

Harrison, X. A. 2014. Using observation-level random effects to model overdispersion in count data in ecology and evolution. PeerJ 2:e616.

https://peerj.com/articles/616/


Y ~ NB(lambda, alpha)
Var(Y) = lambda + lambda^2 / alpha
